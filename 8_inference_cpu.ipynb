{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z37QXCy6KcK",
        "outputId": "2ef1efb9-8c80-4f2e-94d6-edeac1da13d0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# --- 1. SETUP PATHS ---\n",
        "# Your fine-tuned model path on HF\n",
        "FINE_TUNED_REPO = \"omarmosleh/spark-tts-merged-dataset\" \n",
        "# The base model for architecture config and tokenizer\n",
        "BASE_MODEL_REPO = \"SparkAudio/Spark-TTS-0.5B\" \n",
        "BASE_DIR = \"Spark-TTS-0.5B\"\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "\n",
        "# --- 2. DOWNLOAD BASE ASSETS (Config/Tokenizer) ---\n",
        "# We need the official config/tokenizer because fine-tuned repos often lack them\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"‚è≥ Downloading base assets from {BASE_MODEL_REPO}...\")\n",
        "    snapshot_download(repo_id=BASE_MODEL_REPO, local_dir=BASE_DIR, token=os.environ[\"HF_TOKEN\"])\n",
        "\n",
        "# --- 3. INITIALIZE COMPONENTS ---\n",
        "print(\"‚è≥ Loading model with fine-tuned weights...\")\n",
        "\n",
        "# Load official config to force 0.5B architecture dimensions\n",
        "config = AutoConfig.from_pretrained(os.path.join(BASE_DIR, \"LLM\"), trust_remote_code=True)\n",
        "\n",
        "# Load official tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(os.path.join(BASE_DIR, \"LLM\"), trust_remote_code=True)\n",
        "\n",
        "# Load your FINE-TUNED weights from HF using the official config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    FINE_TUNED_REPO,\n",
        "    config=config,\n",
        "    trust_remote_code=True,\n",
        "    dtype=torch.float32,\n",
        "    device_map={\"\": \"cpu\"}, # Force CPU\n",
        "    token=os.environ[\"HF_TOKEN\"]\n",
        ")\n",
        "\n",
        "# Load Audio Tokenizer (The Decoder)\n",
        "audio_tokenizer = BiCodecTokenizer(BASE_DIR, \"cpu\")\n",
        "\n",
        "print(\"‚úÖ System Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dKdC_nL6S_R",
        "outputId": "1d66fd40-34b0-4ac7-a9c2-ec7047048a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Re-aligning model architecture for CPU...\n",
            "Missing tensor: mel_transformer.spectrogram.window\n",
            "Missing tensor: mel_transformer.mel_scale.fb\n",
            "‚úÖ Model loaded with 0.5B dimensions. Try running your inference script now.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- 4. INFERENCE FUNCTION ---\n",
        "@torch.inference_mode()\n",
        "def generate_speech(text):\n",
        "    prompt = f\"<|task_tts|><|start_content|>{text}<|end_content|><|start_global_token|>\"\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cpu\")\n",
        "    \n",
        "    # Generate tokens\n",
        "    gen_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=True, temperature=0.4)\n",
        "    gen_text = tokenizer.batch_decode(gen_ids[:, inputs.input_ids.shape[1]:], skip_special_tokens=False)[0]\n",
        "    \n",
        "    # Extract IDs using Regex\n",
        "    s_tokens = torch.tensor([int(t) for t in re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", gen_text)]).long().unsqueeze(0)\n",
        "    g_tokens = torch.tensor([int(t) for t in re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", gen_text)]).long().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    if s_tokens.shape[1] == 0: return None\n",
        "\n",
        "    # Detokenize to Audio\n",
        "    return audio_tokenizer.detokenize(g_tokens.squeeze(0), s_tokens)\n",
        "\n",
        "# --- 5. EXECUTION ---\n",
        "input_text = \"ŸáŸÜŸàŸàÿØÿ©ÿå ŸáÿßÿØ ÿßŸÑÿµŸàÿ™ ŸÖÿπŸÖŸàŸÑ ŸÉÿßŸÖŸÑÿß ÿ®ÿßŸÑ AI\"\n",
        "wav = generate_speech(input_text)\n",
        "\n",
        "if wav is not None:\n",
        "    import soundfile as sf\n",
        "    sf.write(\"output.wav\", wav, 16000)\n",
        "    print(\"‚úÖ Success! Created output.wav\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to generate tokens.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "3BxEIb3u-cvl",
        "outputId": "cd4d89b2-5931-402d-83d6-682732173559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating speech for: 'ŸáŸÜŸàŸàÿØÿ©ÿå ŸáÿßÿØ ÿßŸÑÿµŸàÿ™ ŸÖÿπŸÖŸàŸÑ ŸÉÿßŸÖŸÑÿß ÿ®ÿßŸÑ AI'\n",
            "Generating token sequence...\n",
            "Token sequence generated.\n",
            "Found 346 semantic tokens.\n",
            "Found 3 global tokens.\n",
            "Detokenizing audio tokens...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x384 and 4096x1024)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2059059649.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generating speech for: '{input_text}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{chosen_voice}: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchosen_voice\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mgenerated_waveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_speech_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgenerated_waveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2059059649.py\u001b[0m in \u001b[0;36mgenerate_speech_from_text\u001b[0;34m(text, temperature, top_k, top_p, max_new_audio_tokens, device)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0maudio_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Squeeze the extra dimension from global tokens as seen in SparkTTS example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     wav_np = audio_tokenizer.detokenize(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mpred_global_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Shape (1, N_global)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mpred_semantic_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# Shape (1, N_semantic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Spark-TTS/sparktts/models/audio_tokenizer.py\u001b[0m in \u001b[0;36mdetokenize\u001b[0;34m(self, global_tokens, semantic_tokens)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \"\"\"\n\u001b[1;32m    144\u001b[0m         \u001b[0mglobal_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mwav_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwav_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Spark-TTS/sparktts/models/bicodec.py\u001b[0m in \u001b[0;36mdetokenize\u001b[0;34m(self, semantic_tokens, global_tokens)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m    183\u001b[0m         \u001b[0mz_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0md_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeaker_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Spark-TTS/sparktts/modules/speaker/speaker_encoder.py\u001b[0m in \u001b[0;36mdetokenize\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mzq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_from_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0md_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x384 and 4096x1024)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OUTPUT_DIR = \"fulldataset\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_speech_enhanced(text, file_prefix=\"01\"):\n",
        "    \"\"\"\n",
        "    Generates speech and provides detailed logs of the tokenization process.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüöÄ Starting Inference for: '{text}'\")\n",
        "    \n",
        "    # 1. Prepare Prompt\n",
        "    prompt = f\"<|task_tts|><|start_content|>{text}<|end_content|><|start_global_token|>\"\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cpu\")\n",
        "    \n",
        "    # 2. Generate Tokens from LLM\n",
        "    print(\"‚è≥ Generating tokens from LLM (this may take a moment on CPU)...\")\n",
        "    gen_ids = model.generate(\n",
        "        **inputs, \n",
        "        max_new_tokens=1024, \n",
        "        do_sample=True, \n",
        "        temperature=0.4,\n",
        "        top_k=50\n",
        "    )\n",
        "    \n",
        "    # 3. Decode and Parse\n",
        "    # We skip special tokens = False because we NEED to see the <|bicodec_...|> tags\n",
        "    gen_text = tokenizer.batch_decode(gen_ids[:, inputs.input_ids.shape[1]:], skip_special_tokens=False)[0]\n",
        "    \n",
        "    # Extract Semantic and Global tokens using regex\n",
        "    semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", gen_text)\n",
        "    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", gen_text)\n",
        "    \n",
        "    s_tokens = torch.tensor([int(t) for t in semantic_matches]).long().unsqueeze(0)\n",
        "    g_tokens = torch.tensor([int(t) for t in global_matches]).long().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # --- METADATA LOGGING ---\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"üìä INFERENCE STATISTICS:\")\n",
        "    print(f\"üîπ Global Tokens Generated:   {len(global_matches)}\")\n",
        "    print(f\"üîπ Semantic Tokens Generated: {len(semantic_matches)}\")\n",
        "    \n",
        "    if s_tokens.shape[1] == 0:\n",
        "        print(\"‚ùå Error: No semantic tokens were generated. Try a different prompt.\")\n",
        "        return None\n",
        "\n",
        "    # 4. Detokenize to Audio (The Codec Step)\n",
        "    print(\"üîä Converting tokens to waveform via BiCodec...\")\n",
        "    wav = audio_tokenizer.detokenize(g_tokens.squeeze(0), s_tokens)\n",
        "    \n",
        "    # 5. Save and Export\n",
        "    # Clean text for filename (remove special characters)\n",
        "    clean_text = re.sub(r'[^\\w\\s-]', '', text).strip().replace(' ', '_')\n",
        "    filename = f\"{file_prefix}_{clean_text}.wav\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    \n",
        "    sample_rate = 16000 # Standard for Spark-TTS\n",
        "    sf.write(filepath, wav, sample_rate)\n",
        "    \n",
        "    file_size_kb = os.path.getsize(filepath) / 1024\n",
        "    print(f\"‚úÖ Audio saved to: {filepath}\")\n",
        "    print(f\"üìè Output Size: {file_size_kb:.2f} KB\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # 6. Play in Notebook\n",
        "    display(Audio(wav, rate=sample_rate))\n",
        "    \n",
        "    return filepath\n",
        "\n",
        "# --- EXECUTION ---\n",
        "# You can manually change the prefix (the asterisk replacement)\n",
        "manual_prefix = \"001\" \n",
        "input_text = \"ÿ¥Ÿà ÿßŸÑÿ£ÿÆÿ®ÿßÿ±ÿü ÿ∑ŸÖŸÜŸÜŸä ÿπŸÜŸÉÿå ÿßŸÜÿ¥ÿßŸÑŸÑŸá ÿ®ÿÆŸäÿ±ÿü\"\n",
        "\n",
        "output_path = generate_speech_enhanced(input_text, file_prefix=manual_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUxMJr1MCJU6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
